{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# day1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b92039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras # 케라스 호출 \n",
    "from tensorflow.keras import layers # 케라스 내부 레이어 호출\n",
    "from tensorflow.keras.models import Sequential # 케라스 모델 시퀸셜 바로 호출\n",
    "from tensorflow.keras.layers import Dense # 케라스 레이어 덴스 바로 호출\n",
    "from tensorflow.keras.datasets import mnist # mnist 데이터셋(숫자훈련 데이터셋)\n",
    "from tensorflow.keras.utils import to_categorical # 유틸을 카테고리로 변경하는 모듈\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping # 모델 체크포인트, 조기중단 클래스\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN 모델 정의\n",
    "model = keras.Sequential([layers.Dense(units=1,input_shape=[3])]) \n",
    "# keras.Sequential() : 케라스에서 신경망을 구축하는데 사용되는 클래스. \n",
    "# layers.Dense : 완전연결신경망(fully connected layers)으로 설정\n",
    "# units=1 : 출력값\n",
    "# input_shape=[3] : 3개의 특성 투입\n",
    "\n",
    "model = keras.Sequential([                                 # 신경망 구축 \n",
    "layers.Dense(units=4, input_shape=[2], activation='relu'), # 출력값 = 4, 입력값 = 2, 활성화함수 = 렐루 \n",
    "layers.Dense(units=3, activation='relu'),                  # 출력값 = 3, 활성화함수 = 렐루\n",
    "layers.Dense(units=1, activation='sigmoid')                # 출력값 = 1, 활성화함수 = 시그모이드\n",
    "])\n",
    "# 활성함수: 렐루(0~1까지의 숫자를 출력하며 널리 쓰임)\n",
    "#           시그모이드(-1~1까지의 숫자를 출력하며 최종본 출력때 쓰임)\n",
    "#           소프트맥스(원핫인코딩을 한 경우 자주 사용됨)\n",
    "\n",
    "model.compile(optimizer='adam',loss='mae') # 모델을 재구성한다. 옵티마이저 : 아담, 손실함수 mae (mse나 rmse도 가능)\n",
    "# 옵티마이저 종류 : 그레디언트 디센트 알고리즘, 아담 옵티마이저, rms prof, momentum등이 존재.\n",
    "model.summary() # 현재 만들어진 모델의 구조를 나타낸다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b4cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스 모델. 추가형식\n",
    "model = Sequential() #모델 정의\n",
    "model.add(Dense(12,input_dim=8,activation='relu', name='Dense_1')) # 만들어진 모델에 끼워서 추가하기 \n",
    "model.add(Dense(8,activation='relu'))                              # dim은 바로 숫자를 쓰게 해준다. \n",
    "model.add(Dense(1,activation='sigmoid')                            # 이름을 추가할 수 있다. \n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])# 모델 재구성\n",
    "          # 옵티마이저 : 아담, 손실함수: 이진교차엔트로피 ,매트릭스 : 정확도 중심\n",
    "# 손실함수 설정 \n",
    "# 결과가 0 또는 1로 나오는 경우 : 이진교차 엔트로피 'binary_crossentropy'\n",
    "# 결과가 연속형 형식으로 나오는 경우(가격 등) : 평균제곱오차 'mean_squared_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68595eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스 모델 사용하기 \n",
    "model.fit(x_train, y_train, epochs=10,batch_size=5) # 모델적용 : \n",
    "    # 훈련데이터, 답지, 에폭스 : 반복훈련횟수, 배치사이즈(한번에 투입하는 사이즈)\n",
    "model.predict(np.array([[5.9,3.0,5.1,1.8]])) # 모델에 답 투입하여 샘플 1개 확인하기.(인풋에 맞게 투입)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일 / 그래프로 보기\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))                \n",
    "model.add(Dense(8, activation='relu'))                 \n",
    "model.add(Dense(1, activation='sigmoid'))  \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25) \n",
    "# 히스토리 항목에 저장 / 검증 분할 0.25\n",
    "hist_df=pd.DataFrame(history.history)      \n",
    "# 히스토리 항목 데이터프레임으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9211b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss=hist_df['val_loss']                                                 # val_loss 항목을 y축 밸류로스 로 설정\n",
    "y_loss=hist_df['loss']                                                      # loss항목을 y축 로스 항목으로 설정\n",
    "x_len = np.arange(len(y_loss))                                              # y로스의 길이만큼 x의 길이로 설정\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2, label='Testset_loss')  # x의 길이와 y축의 밸류로스(테스트셋)을 빨간색으로 표시\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=2, label='Trainset_loss') # x의 길이와 y축의 로스(훈련셋)을 파란색으로 표시\n",
    "\n",
    "plt.legend(loc='upper right')                                               # 플랏리 보여주기\n",
    "plt.xlabel('epoch')                                                         # 플랏리x라벨은 에폭\n",
    "plt.ylabel('loss')                                                          # 플랏리 y라벨은 로스\n",
    "plt.show()                                                                  # 그래프 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13cf8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 자동중단\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping # 모델 체크포인트, 조기중단 클래스\n",
    "early_stopping_callback = EarlyStopping(                              # 자동중단기능\n",
    "    monitor='val_loss', patience=20)                                  # 모니터링 : 밸류로스, 대기횟수 20\n",
    "modelpath=\"./data/model/Ch14-4-bestmodel.hdf5\"                        # 모델저장위치 확인\n",
    "checkpointer = ModelCheckpoint(                                       # 모델 저장 체크포인트\n",
    "    filepath=modelpath, monitor='val_loss',                           # 파일저장위치, 모니터링:밸류로스\n",
    "    verbose=0, save_best_only=True)                                   # 알림기능 끔, 최고상태에서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b25674",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(X_train, y_train,                                   # 모델 훈련\n",
    "                  epochs=2000, batch_size=500,                        # 반복횟수 2천, 배치사이즈200\n",
    "                  validation_split=0.25, verbose=1,                   # 자료나눔 25% , 알림 활성화\n",
    "                  callbacks=[early_stopping_callback,checkpointer])   # 콜백함수, 자동중단, 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8276739",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)                                         # 모델 실행, 정확도 확인\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(testX, testy)[1]))   # 모델정확도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050180d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss='categorical_crossentropy'                    # 손실함수 : 범주형 교차 엔트로피"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c43f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장/불러오기\n",
    "model.save('myModel.hdf5')                                        # 모델 저장\n",
    "del model                                                         # 모델 제거\n",
    "load_model('myModel.hdf5')                                        # 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f91b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2일차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd74f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential                                    # 모델 불러오기\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D # 덴스, 드롭아웃,평탄화, 콘볼루션2d, 풀링2d\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint             # 최적위치 저장용\n",
    "from sklearn.model_selection import train_test_split                              # 훈련 / 테스트 데이터 분할용\n",
    "from keras.datasets import fashion_mnist                                          # 패션데이터\n",
    "from tensorflow.keras.utils import to_categorical                                 # 유틸자료 카테고리로 바꾸기\n",
    "import matplotlib.pyplot as plt                                 \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()  # 훈련셋, 테스트셋 다운로드. \n",
    "print(\"학습셋 이미지 수 : %d 개\" % (trainX.shape[0]))         # 연습세트 갯수 확인\n",
    "print(\"테스트셋 이미지 수 : %d 개\" % (testX.shape[0]))        # 테스트셋 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b456b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사진가공 기준데이터 28*28=784\n",
    "trainX = trainX.reshape(trainX.shape[0], 784).astype('float32') / 255  # RGB개수만큼 나눠서 정규화\n",
    "testX = testX.reshape(testX.shape[0], 784).astype('float32') / 255     # 위와 동일\n",
    "trainy = to_categorical(trainy, 10)                                    # 유틸형식 연습데이터 카테고리형식 변경\n",
    "testy = to_categorical(testy, 10)                                      # 위와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6e1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 딥러닝 제작\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu')) # 필터갯수, 필터크기, 투입사진, 작동방식\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))      # 필터갯수, 필터크기(생략되었다), 작동방식\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))              # 풀링하기.2,2사이즈로 만들기\n",
    "model.add(Dropout(0.25))                              # 25%를 드롭아웃한 상태에서 진행\n",
    "model.add(Flatten())                                  # 플래튼이 일어난 다음에 히든계층으로 간다. \n",
    "model.add(Dense(128,  activation='relu'))             # 128차원짜리 층으로 나눔\n",
    "model.add(Dropout(0.5))                               # 50% 드롭\n",
    "model.add(Dense(10, activation='softmax'))            # 소프트맥스로 결과값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이후 동일\n",
    "model.compile(loss='categorical_crossentropy',           # 범주형 교차 엔트로피\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "modelpath=\"./data/model/MNIST_CNN.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3일차. 딥러닝 이미지 조절\n",
    "import tensorflow as tf                              # 텐서플로 호출\n",
    "from tensorflow.keras.preprocessing import image     # 케라스 이미지 호출\n",
    "import numpy as np                                   # 넘파이 호출                               \n",
    "import matplotlib.pyplot as plt                      # 파이플랏\n",
    "from tensorflow.image import pad_to_bounding_box     # 이미지 묶기\n",
    "from tensorflow.image import central_crop            # 가운데 추출\n",
    "from tensorflow.image import resize                  # 이미지 크기조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 조절 기술\n",
    "bgd = image.load_img('dogs.png')                     # 이미지 불러오기\n",
    "image.img_to_array(bgd)                              # 이미지 배열화\n",
    "image.img_to_array(bgd).shape                        # 이미지 배열화,(높이, 너비, 채널)\n",
    "bgd_vector = np.asarray(image.img_to_array(bgd))     # 이미지벡터에 전달 \n",
    "bgd_vector = bgd_vector/255                          # 이미지 벡터 정규화\n",
    "\n",
    "plt.imshow(bgd_vector)                               # 플랏에 이미지 이미지 보여주기\n",
    "plt.show()                                           # 이미지 보여주기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 크기 변경하기 - 테두리 이미지 설정\n",
    "target_height = 4500                                 # 변경할 이미지 높이           \n",
    "target_width = 4500                                  # 변경할 이미지 폭\n",
    "\n",
    "source_height = bgd_vector.shape[0]                  # 현재 이미지의 높이\n",
    "source_width = bgd_vector.shape[1]                   # 현재 이미지의 넓이\n",
    " \n",
    "# padding 실시 : pad_to_bounding_box 사용 \n",
    "bgd_vector_pad = pad_to_bounding_box(bgd_vector,                           # 패드를 박스로 묶기 \n",
    "                                     int((target_height-source_height)/2), # 타겟높이-설정폭을 2로 나눠서 정수화\n",
    "                                     int((target_width-source_width)/2),   # 타겟폭-설정폭을  2로 나눠 정수화\n",
    "                                     target_height,                        # 변경할 이미지 높이\n",
    "                                     target_width)                         # 변경할 이미지 폭\n",
    "                                     \n",
    "bgd_vector_pad.shape                                 # 이미지 형태 확인\n",
    "plt.imshow(bgd_vector_pad)                           # 플랏에 이미지 보여주기\n",
    "plt.show()                                           # 이미지 보여주기\n",
    "   \n",
    "#이미지 저장 \n",
    "image.save_img('dogs_pad.png', bgd_vector_pad)       # 이미지 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f67f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 추출하기\n",
    "bgd_vector_crop = central_crop(bgd_vector, .5)       # 가운데만 추출하기, 추출대상, 50% 추출\n",
    "bgd_vector_crop.shape                                # 추출 이미지 형태확인\n",
    " \n",
    "plt.imshow(bgd_vector_crop)                          # 플랏에 이미지 보여주기  \n",
    "plt.show()                                           # 이미지 보여주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28828148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 사이즈 다시 정의하기\n",
    "bgd_vector_resize = resize(bgd_vector, (300,300))    # 원본사진크기 재설정하기. 300*300으로 재설정\n",
    "bgd_vector_resize.shape                              # 재설정 이미지 형태확인\n",
    " \n",
    "plt.imshow(bgd_vector_resize)                        # 플랏에 이미지 보여주기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf45caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 사이즈 정사각형으로 정의하기\n",
    "w, h = img.size                                      # 이미지 사이즈x,y,축으로 나눠서 대입\n",
    "s = min(w, h)                                        # 둘 중 최소사이즈 확인\n",
    "img = img.crop((x, y, x+s, y+s))                     # 최소사이즈를 기준으로 이미지 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0792b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습 모델 불러오기 : 케라스에서 클래스 형태로 제공함 \n",
    "from tensorflow.keras.applications.vgg16 import VGG16  # 사전학습된 모델 불러오기 VGG16\n",
    "model = VGG16(weights='imagenet', include_top=True)    # weight, include_top 파라미터 설정 \n",
    "model.summary()                                        # 모델 설계구조 확인\n",
    "\n",
    "# from tensorflow.keras.applications  import  여기서 탭을 누르면 역대 우승모델들을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input            #  vgg16모델에 인풋 모듈\n",
    "from tensorflow.keras.preprocessing import image                            # 이미지 모듈\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image                                                       # PIL이미지\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions # 이미지 분류결과 확률로 표시하기.\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input   # 이미지 인풋용 모듈\n",
    "import numpy as np    \n",
    " \n",
    "img = Image.open('dogs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b839d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다운로드 모델 사용하기\n",
    "model.layers[0].input_shape                          # 인풋 가능한 기준 보기\n",
    "target_size = 224                                    # 이미지 크기숫자 지정\n",
    "img = img.resize((target_size, target_size))         # 이미지 크기를 인풋 가능한 수치로 변경\n",
    "np.expand_dims(np_img, axis=0)                       # 이미지를 1배치로 지정\n",
    "pre_processed = preprocess_input(img_batch)          # 이미지 인풋용 모듈에 이미지 투입\n",
    "y_preds = model.predict(pre_processed)               # 이미지 답안체크\n",
    "np.set_printoptions(                                 # 부동소숫점 숫자출력옵션\n",
    "    suppress=True, precision=10)                     # 과학적 표기법 비활성화, 출력소숫점 자리수 10\n",
    "np.max(y_preds)                                      # 모델 내 최고의 확률값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install albumentations                         # 앨범엔테이션. 이미지 회전용 모듈                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e59188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 증식요령\n",
    "image_tensor = tf.keras.preprocessing.image.img_to_array(image)           # 넘파이배열로 전환한 이미지를 텐서로 전환\n",
    "\n",
    "# 이미지 좌우반전\n",
    "flip_lr_tensor = tf.image.flip_left_right(image_tensor)                   # 이미지 좌우반전\n",
    "flip_ud_tensor = tf.image.flip_up_down(image_tensor)                      # 이미지 상하반전\n",
    "flip_lr_image = tf.keras.preprocessing.image.array_to_img(flip_lr_tensor) # 배열을 이미지로 변경\n",
    "flip_ud_image = tf.keras.preprocessing.image.array_to_img(flip_ud_tensor) # 배열을 이미지로 변경\n",
    "flip_lr_tensor = tf.image.random_flip_left_right(image_tensor)            # 이미지 랜덤확률로 좌우반전\n",
    "flip_ud_tensor = tf.image.random_flip_up_down(image_tensor)               # 이미지 랜덤확률로 상하반전\n",
    "\n",
    "# 이미지 중간추출\n",
    "cropped_tensor = tf.image.central_crop(image_tensor, frac)                # 이미지 중간 추출(이미지, 확대비율)\n",
    "cropped_img = tf.keras.preprocessing.image.array_to_img(cropped_tensor)   # 추출그림 이미지로 전환\n",
    "\n",
    "# 이미지 밝기조절\n",
    "random_bright_tensor = tf.image.random_brightness(image_tensor, max_delta=128)       # 이미지 밝기 -128~128 중 랜덤설정 \n",
    "random_bright_tensor = tf.clip_by_value(random_bright_tensor, 0, 255)                # 이미지 데이터의 밝기를 0~255사이로 제한\n",
    "random_bright_image = tf.keras.preprocessing.image.array_to_img(random_bright_tensor)# 추출그림 이미지로 전환\n",
    "\n",
    "import albumentations as A\n",
    "# 이미지 회전\n",
    "transform = A.Compose([                                                   # albumentations 블록 정의하기\n",
    "    A.Affine(rotate=(-45,45),                                             # 아핀 변환 적용, 이미지회전 -45~45도,\n",
    "             scale=(0.5,0.9), p=0.5)])                                    # 이미지 크기 0.5~0.9사이, 변환적용확률 0.5\n",
    "    transformed = transform(image=image_arr)                              # 이미지 배열을 블록에 정의한대로 무작이 변환 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이낸스 리더 패키지 주가 예측모델 기초\n",
    "import FinanceDataReader as fdr                      # 파이낸스 데이터 리더   \n",
    "# 안되면 pip install finance-datareader를 설치\n",
    "from keras.models import Sequential                  # 케라스 모델\n",
    "from keras.layers import Dense                       # 전체연결\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint # 스톱모듈\n",
    "from keras.layers import LSTM                        # LSTM모듈\n",
    "from sklearn.preprocessing import MinMaxScaler       # 정규화 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c017be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fdr.DataReader('APPL')                          # 애플 데이터 호출\n",
    "fdr.DataReader('AAPL','1981','1982')                 # 원하는 기간의 애플 데이터 호출\n",
    "fdr.DataReader('AAPL','1981-01-01','1982-12-30')     # 원하는 날짜 사이의 애플 데이터 호출\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(        # 종가 외의 정보가 문제지. 종가가 답안\n",
    "    df.drop(\"Close\",axis = 1),df['Close'], test_size=0.2,\n",
    "    random_state=0, shuffle=False)                   # 시간의 흐름이기 때문에 셔플옵션 끄기 \n",
    "\n",
    "def make_dataset(data, label, window_size=20):                   # 데이터셋 제작함수 윈도우사이즈 : 타임스텝\n",
    "    feature_list = []                                            # 요소리스트\n",
    "    label_list = []                                              # 라벨리스트\n",
    "    for i in range(len(data) - window_size):                     # 데이터 길이에 윈도우의 길이만큼 빼기.   \n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))# 현재부터 20일까지의 해당 날짜의 시고저종*20일치를 배열화.\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))   # 라벨은 20일째의 종가를 담아서 배열화하여 리스트에 추가. \n",
    "    return np.array(feature_list), np.array(label_list)          # (날짜길이 - 20) *20*4를 리턴\n",
    "\n",
    "\n",
    "model.add(LSTM(16, input_shape=(xtrain.shape[1],xtrain.shape[2]),# LSTM으로 정의, 훈련데이터 모양 크기를 투입\n",
    "              activation = 'relu', return_sequences=False))      # 맨 마지막 결과만 리턴\n",
    "model.add(Dense(1))                                              # 마지막 아웃풋, 디폴트 = 리니어\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')        # 모델 컴파일 / 연속형값을 예측하기에 mse를 사용. \n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)        # 5번 체크 \n",
    "checkpoint = ModelCheckpoint('tmp_checkpoint.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto') \n",
    "# 오토 : 컴퓨터에게 권한을 위임\n",
    "xtrain, x_valid, ytrain, y_valid = train_test_split(xtrain, ytrain, test_size=0.2)\n",
    "# 별도의 훈련데이터 세팅\n",
    "history = model.fit(xtrain, ytrain,                    # 훈련제이터 제작\n",
    "                    epochs=200,                        # 반복횟수\n",
    "                    batch_size=16,                     # 배치사이즈\n",
    "                    validation_data=(x_valid, y_valid),# 밸리데이션 데이터(별도 제작할 경우) \n",
    "                    callbacks=[early_stop, checkpoint])# 콜백함수 설정\n",
    "model.load_weights('tmp_checkpoint.h5')                # 모델 호출\n",
    "pred = model.predict(xtest)                            # 답안작성\n",
    "df = pd.DataFrame(scaled, columns=cols)                            \n",
    "xtrain,xtest,ytrain,ytest = train_test_split(df.drop(\"Close\",axis = 1),df['Close'], test_size=0.2, random_state=0, shuffle=False)\n",
    "vsta = np.hstack([xtest[20:],pred])\n",
    "ans = scaler.inverse_transform(vsta)                   # 스케일러 인버스 실행(정규화 되기 전 수치로 되돌림)\n",
    "ans2 = ans[:,1]  \n",
    "ans2[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언어모델 기초\n",
    "from tensorflow.keras.models import Sequential                     # 모델 호출기능\n",
    "from tensorflow.keras.layers import SimpleRNN                      # 단순RNN\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer          # 언어 토큰화\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # 인풋 데이터 길이 조절, 잘라내거나 붙이거나 해서 동일한 길이로 인풋시키는 용도\n",
    "from tensorflow.keras.utils import to_categorical                  # 원핫인코딩을 위한 카테고리화 모듈\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN    # 임베딩, 전체연결, RNN\n",
    "from string import punctuation                                     # 특수문자 모음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05453298",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"경마장에 있는 말이 뛰고 있다\\n그의 말이 법이다\\n가는 말이 고와야 오는 말이 곱다\\n\"\"\"\n",
    "tokenizer = Tokenizer()                                 # 토큰화 모듈 호출\n",
    "tokenizer.fit_on_texts([text])                          # 토큰화 모듈 적합화\n",
    "vocab_size = len(tokenizer.word_index) + 1              # 토큰화 모듈 길이 1을 더한값을 기준 \n",
    "print('단어 집합의 크기 : %d' % vocab_size)             # 집합크기가 12개\n",
    "\n",
    "sequences = list()                                      # 비어있는 리스트 출력\n",
    "for line in text.split('\\n'):                           # 줄바꿈 문자를 기준으로 문장 토큰화\n",
    "    encoded = tokenizer.texts_to_sequences([line])[0]   # 텍스트 시퀀스가 문장을 숫자로 변환시켜서 인코딩(2,3,4,1,5로 인코딩된다)\n",
    "    for i in range(1, len(encoded)):                    # 1부터 4까지를 구간으로 설정\n",
    "        sequence = encoded[:i+1]                        # 시퀀스 추출\n",
    "        sequences.append(sequence)                      # 시퀀스 대입\n",
    "print('학습에 사용할 샘플의 개수: %d' % len(sequences)) # 샘플갯수 확인 \n",
    "\n",
    "max_len = max(len(l) for l in sequences)                # 모든 샘플에서 길이가 가장 긴 샘플의 길이 확인\n",
    "sequences = pad_sequences(                              # 시퀀스 패딩(0으로 채우기)\n",
    "    sequences, maxlen=max_len, padding='pre')           # 시퀀스를 샘플 최대 길이 기준으로 채우기(뒤쪽에 채우기, 앞쪽은 'post')\n",
    "sequences = np.array(sequences)                         # 시퀀스 어레이화\n",
    "X = sequences[:,:-1]                                    # 마지막 칸 제외 사용\n",
    "y = sequences[:,-1]                                     # 마지막 칸 사용\n",
    "y = to_categorical(y, num_classes=vocab_size)           # 정답 카테고리화(원 핫 인코딩)\n",
    "\n",
    "embedding_dim = 10                                      # 임베딩 차원 10차원\n",
    "hidden_units = 32                                       # 히든레이어 정의\n",
    "\n",
    "model = Sequential()                                    # 딥러닝 모델 정의\n",
    "model.add(Embedding(vocab_size, embedding_dim))         # 보켑 사이즈를 임베딩차원으로 축소추가\n",
    "model.add(SimpleRNN(hidden_units))                      # 중간 히든계층\n",
    "model.add(Dense(vocab_size, activation='softmax'))      # 소프트맥스로 나눠서 출력\n",
    "model.compile(loss='categorical_crossentropy',          # 분류교차검증\n",
    "              optimizer='adam', metrics=['accuracy'])   # 아담, 정확도 기준\n",
    "model.fit(X, y, epochs=200, verbose=2)                  # 출력 2형태, 200번 반복, x,y로 훈련, 출력메세지 2형\n",
    "\n",
    "def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word                                # 기존 단어/ 문장\n",
    "    sentence = ''                                           # 앞으로 붙일 문장(빈 상태로 생성)\n",
    "    for _ in range(n):\n",
    "                                                                    # 현재 단어에 대한 정수 인코딩과 패딩\n",
    "        encoded = tokenizer.texts_to_sequences([current_word])[0]   # 텍스트를 시퀀스로 변환하기(숫자)\n",
    "        encoded = pad_sequences([encoded], maxlen=5, padding='pre') # 숫자를 기준으로 패딩을 함.길이는 전부 동일\n",
    "        print(encoded[0])\n",
    "                                                                    # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
    "        result = model.predict(encoded, verbose=0)                  # 시퀀스를 모델에 투입\n",
    "        result = np.argmax(result, axis=1)                          # 모델 확률중 가장 큰 확률값을 결과로 반환\n",
    "\n",
    "        for word, index in tokenizer.word_index.items():            # 토큰 요소 중 단어인덱스를 추출\n",
    "                                                                    # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break \n",
    "            if index == result:                                     # 인덱스가 만약 결과값과 똑같으면\n",
    "                break                                               # 끝내기.\n",
    "\n",
    "                                                                    # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        current_word = current_word + ' '  + word                   # 예측단어.\n",
    "        print(current_word)\n",
    "                                                                    # 예측 단어를 문장에 저장\n",
    "        sentence = sentence + ' ' + word                            # 최종단계가 아니면 여기에 저장. \n",
    "        print(sentence)\n",
    "    sentence = init_word + sentence                                 # 기존 단어/문장\n",
    "    return sentence                                                 # 예측된 단어가 붙은 문장\n",
    "print(sentence_generation(model, tokenizer, '경마장에', 4))         # 최종 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1055fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기타 기술  - 특수문자, 언어 결측값 제거\n",
    "headline = [word for word in headline if word != \"Unknown\"]# 문자로 되어 제거가 되지않은 결측치 제거\n",
    "def repreprocessing(raw_sentence):\n",
    "    preproceseed_sentence = raw_sentence.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    # 구두점 제거와 동시에 소문자화\n",
    "    return ''.join(word for word in preproceseed_sentence if word not in punctuation).lower()  \n",
    "    #  punctuation은 특수문자들의 모음으로 특수문자들을 한번에 확인할때 사용\n",
    "    # 특수문자를 한번에 제거\n",
    "preprocessed_headline = [repreprocessing(x) for x in headline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea877ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 깃허브 자료 이용 구문분석(XML파일 분석요령)\n",
    "import re                                                # 정규표현식 \n",
    "import urllib.request                                    # 웹 접속하여 리소스 가져오기\n",
    "import zipfile                                           # 압축파일 해제 모듈\n",
    "from lxml import etree                                   # lxml 분석기\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize   # 단어토큰화 작업용\n",
    "from gensim.models import Word2Vec                       # 워드 투 벡터 \n",
    "from gensim.models import KeyedVectors                   # 워드 투 벡터 모델호출기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac02cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")\n",
    "# 웹 접속하여 리소스 가져오기 / 사이트, 파일명 설정\n",
    "targetXML =open('ted_en-20160408.xml', 'r', encoding= 'utf-8')#XML파일 오픈\n",
    "target_text = etree.parse(targetXML)                          # 파일 내부 텍스트 분할\n",
    "parse_text = '\\n'.join(target_text.xpath(\"//content/text()\")) # <content>본문</content>만 추출\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)           # 괄호문자 제거(괄호 내부까지)\n",
    "sent_text = sent_tokenize(content_text)                       # 텍스트 토큰화\n",
    "\n",
    "normalized_text = []                                          # 리스트 제작\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())      # 소문자로 전부 변경이후 소문자와 숫자 모두를 제외한 나머지를 제거  \n",
    "     normalized_text.append(tokens)                           # 텍스트를 리스트에 추가\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text] # 문자 토큰화     \n",
    "model.wv.save_word2vec_format('eng_w2v')                      # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format('eng_w2v')   # 모델 불러오기\n",
    "loaded_model.most_similar('money')                            # 모델 내 특정단어 유사도 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기타. 토큰화 작업용 tqdm\n",
    "from tqdm import tqdm                                                     # tqdm 호출\n",
    "tokenized_data = []\n",
    "for sentence in tqdm(train_data['document']):                             # tqdm을 데이터로 입력하여 1개씩 반복한다. \n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True)                  # 토큰화. 스템은 True다. \n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거부. \n",
    "    tokenized_data.append(stopwords_removed_sentence)                     # 이중 리스트로 저장하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f060f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기타. api 로드전용\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')             # api 로드\n",
    "wv.vectors.shape                                      # 벡터모양 확인\n",
    "wv.similarity(\"post\",\"book\")                          # 모델 내 특정단어 유사도 체크\n",
    "model.wv.save_word2vec_format(\"kor_w2v\")              # 모델저장\n",
    "!python -m gensim.scripts.word2vec2tensor --input kor_w2v --output kor_w2v # 모델 tsv 변환"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
